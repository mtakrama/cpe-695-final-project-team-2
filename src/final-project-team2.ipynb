{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utilities to provide basic I/O and globally required global functions.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import csv\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class FileType(Enum):\n",
    "    CSV = 1\n",
    "    XLS = 2\n",
    "    XLSX = 3\n",
    "\n",
    "\n",
    "def read_from_file(file_path, file_type=FileType.CSV):\n",
    "    if file_path is None:\n",
    "        raise Exception(\n",
    "            \"Filepath [{}] cannot be none (null)\".format(file_path))\n",
    "    if len(file_path) == 0:\n",
    "        raise Exception(\"Filepath [{}] cannot be empty\".format(file_path))\n",
    "    if not os.path.exists(file_path):\n",
    "        raise Exception(\"Filepath [{}] does not exist.\".format(file_path))\n",
    "\n",
    "    if file_type == FileType.CSV:\n",
    "        return list(csv.reader(open(file_path)))\n",
    "\n",
    "    raise Exception(\n",
    "        \"FileType [{}] for file [{}] unsupported\".format(file_type, file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_970169/3542535800.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdata_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mplot_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'plots'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Daily case trend dedicated parser.\n",
    "\"\"\"\n",
    "\n",
    "import shutil\n",
    "import re\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "data_prefix = '../data'\n",
    "plot_path = 'plots'\n",
    "\n",
    "xaxis_plotter = []\n",
    "filename_plotter = []\n",
    "\n",
    "numPastDays = 40\n",
    "numFutureDays = 20\n",
    "\n",
    "\n",
    "def convert_to_unix(date_str):\n",
    "    eDateTime = datetime.strptime(date_str, '%b %d %Y')\n",
    "    return int(time.mktime(eDateTime.timetuple()))\n",
    "\n",
    "\n",
    "def zeroize_invalid_vaccination_rate(vaccine_rate_str):\n",
    "    if vaccine_rate_str == \"N/A\":\n",
    "        return 0\n",
    "    return vaccine_rate_str\n",
    "\n",
    "\n",
    "def read(file):\n",
    "    try:\n",
    "        data = read_from_file(file, FileType.CSV)\n",
    "        # delete first 3 lines\n",
    "        del data[0:3]\n",
    "\n",
    "        # blow out first column\n",
    "        data = np.array(data)\n",
    "        data = np.delete(data, 0, 1)\n",
    "\n",
    "        # Clean data\n",
    "        for entry in data:\n",
    "            # print('Entry: {}'.format(entry))\n",
    "            entry[0] = convert_to_unix(entry[0])\n",
    "            entry[3] = zeroize_invalid_vaccination_rate(entry[3])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "    # Cast entire array to integer type\n",
    "    return data.astype(np.float64)\n",
    "\n",
    "\n",
    "def plot(filename, title, xlabel, ylabel, xdata, ydata):\n",
    "    plt.figure()\n",
    "    plt.style.use('seaborn-whitegrid')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "    # temporily gen plot color on the fl\n",
    "    r = random.random()\n",
    "    b = random.random()\n",
    "    g = random.random()\n",
    "    color = (r, g, b)\n",
    "\n",
    "    plt.plot(xdata, ydata, c=color)\n",
    "\n",
    "    print(\" - saving \" + filename)\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def setup_plot_paths():\n",
    "    # Cleanup and setup plots path\n",
    "    if os.path.exists(os.path.join(os.getcwd(), plot_path)):\n",
    "        shutil.rmtree(plot_path, ignore_errors=True)\n",
    "\n",
    "    os.mkdir(plot_path)\n",
    "\n",
    "\n",
    "def grab_state_name(path_str):\n",
    "    return re.search('.*__(.+?)\\.csv', path_str).group(1)\n",
    "\n",
    "\n",
    "def plot_all_daily_trends(data_files):\n",
    "    print(\"Plotting daily trends...\")\n",
    "\n",
    "    # setup daily trend path\n",
    "    daily_trend_path = os.path.join(plot_path, 'daily_trends_raw')\n",
    "    os.mkdir(daily_trend_path)\n",
    "\n",
    "    for csv_file in data_files:\n",
    "        daily_cases = read(os.path.join(data_prefix, csv_file))\n",
    "        xaxis_plotter.append(len(daily_cases))\n",
    "\n",
    "        # grab the state name from file path\n",
    "        filename = grab_state_name(csv_file)\n",
    "        filename_plotter.append(filename)\n",
    "\n",
    "        plot(filename=os.path.join(daily_trend_path, filename + \"_raw_daily_case_trends.png\"),\n",
    "             title=\"{} Daily Case Trend\".format(filename.title()),\n",
    "             xlabel=\"Unix Time Stamp\",\n",
    "             ylabel=\"Number of Cases\",\n",
    "             xdata=daily_cases[:, 0],\n",
    "             ydata=daily_cases[:, 2])\n",
    "\n",
    "    print(\"Done plotting daily trends.\")\n",
    "\n",
    "\n",
    "def prepare_data(csv_files):\n",
    "    # These arrays contain the per-day counts for number of cases and new vaccinations,\n",
    "    # and associated Unix timestamps\n",
    "    dataTimestamps = []\n",
    "    dataRawCases = []\n",
    "    dataRawVaccinated = []\n",
    "\n",
    "    # This array contains smaller arrays that have been created as a working set of\n",
    "    # inputs data points for the first layer of the network\n",
    "    dataCuratedX = []\n",
    "\n",
    "    # This array contains smaller arrays that are the expected outputs for the input\n",
    "    # data points given by 'dataCuratedX'\n",
    "    dataCuratedY = []\n",
    "\n",
    "    # State level validation data\n",
    "    state_level_validation_data_lst = []\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        daily_cases = read(os.path.join(data_prefix, csv_file))\n",
    "\n",
    "        current_state = grab_state_name(csv_file)\n",
    "        # print('Extracting data for {} state...'.format(current_state))\n",
    "\n",
    "        state_data_x = []\n",
    "        state_data_y = []\n",
    "        for index, entry in enumerate(daily_cases):\n",
    "            if index > numFutureDays and index + numPastDays < len(daily_cases):\n",
    "                dataTimestamps.append(daily_cases[index, 0])\n",
    "                dataRawCases.append(daily_cases[index, 2])\n",
    "                dataRawVaccinated.append(daily_cases[index, 3])\n",
    "\n",
    "                dataInputX = []\n",
    "                dataInputX.extend(\n",
    "                    daily_cases[index+1:index+numPastDays+1, 2])\n",
    "                dataInputX.extend(\n",
    "                    daily_cases[index+1:index+numPastDays+1, 3])\n",
    "\n",
    "                data_input_x = dataInputX[:]\n",
    "                data_input_y = daily_cases[index-numFutureDays+1:index+1, 2]\n",
    "\n",
    "                dataCuratedX.append(data_input_x)\n",
    "                dataCuratedY.append(data_input_y)\n",
    "\n",
    "                # state level raw data\n",
    "                state_data_x.append(data_input_x)\n",
    "                state_data_y.append(data_input_y)\n",
    "\n",
    "        state_level_split = round(len(state_data_x) * 0.3)\n",
    "        state_level_validation_x = state_data_x[:state_level_split]\n",
    "        state_level_validation_y = state_data_y[:state_level_split]\n",
    "\n",
    "        state_level_validation_data_lst.append(\n",
    "            (current_state, np.array(state_level_validation_x), np.array(state_level_validation_y)))\n",
    "\n",
    "        # print(state_level_validation_data_lst[0][2])\n",
    "\n",
    "        # inspect data\n",
    "        # for state_validation_data in state_level_validation_data_lst:\n",
    "        #     print(state_validation_data)\n",
    "        #     print('Timestamp length: {}'.format(len(state_validation_data[1])))\n",
    "\n",
    "    dataCuratedX = np.array(dataCuratedX)\n",
    "\n",
    "    # Split training and test data\n",
    "    trainingSplitIndex = round(len(dataCuratedX) * 0.3)\n",
    "\n",
    "    dataTrainingTimestamps = dataTimestamps[trainingSplitIndex:]\n",
    "    dataTrainingX = np.asarray(dataCuratedX[trainingSplitIndex:])\n",
    "    dataTrainingY = np.asarray(dataCuratedY[trainingSplitIndex:])\n",
    "\n",
    "    dataTestTimestamps = dataTimestamps[:trainingSplitIndex]\n",
    "    dataTestX = np.asarray(dataCuratedX[:trainingSplitIndex])\n",
    "    dataTestY = np.asarray(dataCuratedY[:trainingSplitIndex])\n",
    "\n",
    "    return dataTimestamps, dataRawCases, dataRawVaccinated, (dataTrainingTimestamps, dataTrainingX, dataTrainingY), (dataTestTimestamps, dataTestX, dataTestY), state_level_validation_data_lst\n",
    "\n",
    "\n",
    "def define_compile_model(optimizer_str, loss_str, metrics_list):\n",
    "    # Build and train model\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(numPastDays * 2, activation=\"relu\"))\n",
    "    model.add(layers.Dense(30, activation=\"relu\"))\n",
    "    model.add(layers.Dense(numFutureDays))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer_str,\n",
    "        loss=loss_str,\n",
    "        metrics=metrics_list\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def plot_loss(history, epochs):\n",
    "    # plot loss\n",
    "    hLoss = history.history['loss']\n",
    "    hVLoss = history.history['val_loss']\n",
    "    plt.figure()\n",
    "    plt.plot(range(epochs), hLoss, '-', range(epochs), hVLoss, '--')\n",
    "    plt.savefig(os.path.join(plot_path, \"loss_plot.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def evaluate_loss_per_state(model, state_level_validation_data_lst):\n",
    "    # model evaluation\n",
    "    state_mse_pairs = []\n",
    "    for state_validation_data in state_level_validation_data_lst:\n",
    "        state_name = state_validation_data[0]\n",
    "\n",
    "        if state_name not in ['idaho', 'mississippi', 'washington', 'delaware']:\n",
    "            continue\n",
    "\n",
    "        eval_info = model.evaluate(\n",
    "            state_validation_data[1], state_validation_data[2])\n",
    "\n",
    "        state_name = state_validation_data[0]\n",
    "        loss_metric = eval_info[0]\n",
    "        mse_metric = eval_info[1]\n",
    "\n",
    "        print('-'*100)\n",
    "        print('State: {}, MSE metric: {}'.format(\n",
    "            state_name, np.sqrt(mse_metric)))\n",
    "\n",
    "        state_mse_pairs.append((state_name, mse_metric))\n",
    "\n",
    "    # plot bar chart\n",
    "    states = [state_mse_pair[0] for state_mse_pair in state_mse_pairs]\n",
    "    rmses = np.sqrt([state_mse_pair[1] for state_mse_pair in state_mse_pairs])\n",
    "\n",
    "    # creating the bar plot\n",
    "    plt.figure()\n",
    "    plt.bar(states, rmses, color='maroon',\n",
    "            width=0.4)\n",
    "    plt.xlabel(\"States\")\n",
    "    plt.ylabel(\"Root Mean Square Error\")\n",
    "    plt.title(\"RMSE per state\")\n",
    "    plt.savefig(os.path.join(plot_path, 'mse_by_state.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "    setup_plot_paths()\n",
    "\n",
    "    # Discover case trend csv files and plot daily trends\n",
    "    csv_files = glob.glob(data_prefix + '/data_table_for_daily_case_trends*')\n",
    "\n",
    "    plot_all_daily_trends(csv_files)\n",
    "\n",
    "    # preparation\n",
    "    training_data_tpl = tuple()\n",
    "    test_data_tpl = tuple()\n",
    "    dataTimestamps, dataRawCases, dataRawVaccinated, training_data_tpl, test_data_tpl, state_level_validation_data_lst = prepare_data(\n",
    "        csv_files)\n",
    "\n",
    "    # model definition\n",
    "    model = define_compile_model(\n",
    "        'adam', 'mean_squared_error', ['mse'])\n",
    "\n",
    "    epochs = 200\n",
    "    history = model.fit(training_data_tpl[1], training_data_tpl[2],\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(test_data_tpl[1], test_data_tpl[2]),\n",
    "                        verbose=0\n",
    "                        )\n",
    "\n",
    "    evaluate_loss_per_state(model, state_level_validation_data_lst)\n",
    "\n",
    "    plot_loss(history, epochs)\n",
    "\n",
    "    prettyplotter(dataRawCases, dataRawVaccinated, test_data_tpl[1], test_data_tpl[2], training_data_tpl[1], training_data_tpl[2],\n",
    "                  training_data_tpl[0], test_data_tpl[0], dataTimestamps, model, numPastDays, numFutureDays)\n",
    "\n",
    "\n",
    "def prettyplotter(dataRawCases, dataRawVaccinated, dataTestX, dataTestY, dataTrainingX, dataTrainingY, dataTrainingTimestamps, dataTestTimestamps, dataTimestamps, model, numPastDays, numFutureDays):\n",
    "\n",
    "    plt.figure()\n",
    "    print('Pretty Model vs training data (error)')\n",
    "    i = 0\n",
    "    accum = numFutureDays\n",
    "    for xaxisTrainPlot in xaxis_plotter:\n",
    "        if i < 10:  # plot first 10\n",
    "            plt.figure()\n",
    "\n",
    "            dataModelInput = []\n",
    "            dataModelInput.extend(dataRawCases[accum:accum+numPastDays])\n",
    "            dataModelInput.extend(dataRawVaccinated[accum:accum+numPastDays])\n",
    "            dataTrainingPredY = model.predict(\n",
    "                np.asarray([dataModelInput[:]]))[0]\n",
    "\n",
    "            plt.plot(dataTimestamps[accum-numFutureDays:accum+numPastDays],\n",
    "                     dataRawCases[accum-numFutureDays:accum+numPastDays], label='Input Case Data')\n",
    "            plt.plot(dataTimestamps[accum-numFutureDays:accum+numPastDays],\n",
    "                     dataRawVaccinated[accum-numFutureDays:accum+numPastDays], label='Input Vaccinated Data')\n",
    "            plt.plot(dataTimestamps[accum-numFutureDays:accum],\n",
    "                     dataTrainingPredY, label='Predicted Data')\n",
    "            plt.title(\"{} Model vs Training Data \".format(\n",
    "                filename_plotter[i].title()))\n",
    "            plt.xlabel(\"Unix Time Stamp\")\n",
    "            plt.ylabel(\"Number of Cases\")\n",
    "            plt.legend(loc=\"upper right\")\n",
    "\n",
    "            accum = accum + xaxisTrainPlot\n",
    "            figtitle = \"pretty_model_vs_training_data_error_\" + \\\n",
    "                str(filename_plotter[i]) + \".png\"\n",
    "            i = i + 1\n",
    "\n",
    "            plt.savefig(os.path.join(\n",
    "                plot_path, figtitle))\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36cf16204b8548560b1c020c4e8fb5b57f0e4c58016f52f2d4be01e192833930"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
